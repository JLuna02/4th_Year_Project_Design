{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883dc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM_FRAMES = 10\n",
    "IMG_SIZE = 112\n",
    "\n",
    "def get_video_paths_and_labels(data_dir):\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for label_index, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.endswith(\".mp4\"):\n",
    "                    video_paths.append(os.path.join(class_dir, fname))\n",
    "                    labels.append(label_index)\n",
    "\n",
    "    return video_paths, labels, class_names\n",
    "\n",
    "# Modify load_video to handle TensorFlow tensors\n",
    "def load_video(path, max_frames=NUM_FRAMES, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    path = path.numpy().decode('utf-8')  # Convert Tensor -> string\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while len(frames) < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame / 255.0  # Normalize\n",
    "            frames.append(frame)\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    # Pad if not enough frames\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(np.zeros((resize[1], resize[0], 3)))\n",
    "\n",
    "    return np.array(frames, dtype=np.float32)\n",
    "\n",
    "# Wrapper for using tf.py_function\n",
    "def load_video_py(path):\n",
    "    video = tf.py_function(func=load_video, inp=[path], Tout=tf.float32)\n",
    "    video.set_shape((NUM_FRAMES, IMG_SIZE, IMG_SIZE, 3))  # Set shape of the returned tensor\n",
    "    return video\n",
    "\n",
    "# Create dataset from video paths and labels\n",
    "def make_dataset(video_paths, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((video_paths, labels))\n",
    "\n",
    "    def _process(path, label):\n",
    "        video = load_video_py(path)  # Call the updated load_video function\n",
    "        return video, label\n",
    "\n",
    "    dataset = dataset.map(_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load paths and labels\n",
    "video_paths, labels, class_names = get_video_paths_and_labels(\"cleaned_dataset\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_ds = make_dataset(train_paths, train_labels).shuffle(100).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = make_dataset(val_paths, val_labels).batch(8).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of one frame in the set of frames created\n",
    "HEIGHT = 112\n",
    "WIDTH = 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a32b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "\n",
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "\n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])\n",
    "\n",
    "\n",
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "\n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "\n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos\n",
    "\n",
    "\n",
    "input_shape = (None, 10, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4)(x)\n",
    "\n",
    "model = keras.Model(input, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28422e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26990f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 50, \n",
    "                    validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model \n",
    "model.save(\"Project_Model/second_chance2.h5\")\n",
    "model.save(\"Project_Model/second_chance4.keras\")\n",
    "model.export(\"saved_model2\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec73abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77188fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "model.evaluate(val_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume `model` is your trained model\n",
    "\n",
    "# Initialize lists to hold true labels and predicted labels\n",
    "y_true_classes = []\n",
    "y_pred_classes = []\n",
    "\n",
    "# Iterate through the validation dataset\n",
    "for video_batch, label_batch in val_ds:\n",
    "    # Make predictions on the batch\n",
    "    y_pred = model.predict(video_batch)\n",
    "\n",
    "    # Get the predicted classes (since it's multi-class, use argmax)\n",
    "    y_pred_classes_batch = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Get the true classes (already in integer form, no need to do argmax)\n",
    "    y_true_classes_batch = label_batch.numpy()\n",
    "\n",
    "    # Append the batch results to the lists\n",
    "    y_true_classes.extend(y_true_classes_batch)\n",
    "    y_pred_classes.extend(y_pred_classes_batch)\n",
    "\n",
    "# Now generate the confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Define your labels (if they are not predefined, replace this with the actual class labels)\n",
    "labels = class_names  # Using the class names from your dataset\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
